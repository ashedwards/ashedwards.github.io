<!DOCTYPE html>
<!-- saved from url=(0036)https://www.cc.gatech.edu/~aedwards/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Ashley Edwards</title>
    
    <meta name="description" content="">
        <meta name="keywords" content="">
            <link href="./Ashley Edwards_files/css" rel="stylesheet">
                <script async="" src="./Ashley Edwards_files/analytics.js"></script><script src="./Ashley Edwards_files/jquery.min.js"></script>
                <script src="./Ashley Edwards_files/config.js"></script>
                <script src="./Ashley Edwards_files/skel.min.js"></script>
                <noscript>
                    <link rel="stylesheet" href="css/skel-noscript.css" />
                    <link rel="stylesheet" href="css/style.css" />
                    <link rel="stylesheet" href="css/style-desktop.css" />
                </noscript>
                <!--[if lte IE 9]><link rel="stylesheet" href="css/ie9.css" /><![endif]-->
                <!--[if lte IE 8]><script src="js/html5shiv.js"></script><link rel="stylesheet" href="css/ie8.css" /><![endif]-->
                <!--[if lte IE 7]><link rel="stylesheet" href="css/ie7.css" /><![endif]-->
                <script>
                    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
                     
                     ga('create', 'UA-61243618-1', 'auto');
                     ga('send', 'pageview');
                     
                    </script><style type="text/css">html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline;}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block;}body{line-height:1;}ol,ul{list-style:none;}blockquote,q{quotes:none;}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none;}table{border-collapse:collapse;border-spacing:0;}body{-webkit-text-size-adjust:none}</style><style type="text/css">.container{width:1200px !important;margin: 0 auto;}</style><style type="text/css">.\31 2u{width:100%}.\31 1u{width:91.6666666667%}.\31 0u{width:83.3333333333%}.\39 u{width:75%}.\38 u{width:66.6666666667%}.\37 u{width:58.3333333333%}.\36 u{width:50%}.\35 u{width:41.6666666667%}.\34 u{width:33.3333333333%}.\33 u{width:25%}.\32 u{width:16.6666666667%}.\31 u{width:8.3333333333%}.\31 u,.\32 u,.\33 u,.\34 u,.\35 u,.\36 u,.\37 u,.\38 u,.\39 u,.\31 0u,.\31 1u,.\31 2u{float:left;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-o-box-sizing:border-box;-ms-box-sizing:border-box;box-sizing:border-box}.\-11u{margin-left:91.6666666667%}.\-10u{margin-left:83.3333333333%}.\-9u{margin-left:75%}.\-8u{margin-left:66.6666666667%}.\-7u{margin-left:58.3333333333%}.\-6u{margin-left:50%}.\-5u{margin-left:41.6666666667%}.\-4u{margin-left:33.3333333333%}.\-3u{margin-left:25%}.\-2u{margin-left:16.6666666667%}.\-1u{margin-left:8.3333333333%}.row.flush{margin-left:0}.row.flush>*{padding:0!important}</style><style type="text/css">.row>*{padding:50px 0 0 50px}.row+.row>*{padding-top:50px}.row{margin-left:-50px}.row.half>*{padding:25px 0 0 25px}.row.half+.row.half>*{padding-top:25px}.row.half{margin-left:-25px}.row.double>*{padding:100px 0 0 100px}.row.double+.row.double>*{padding-top:100px}.row.double{margin-left:-100px}</style><style type="text/css">.row:after{content:'';display:block;clear:both;height:0}.row:first-child>*{padding-top:0}.row>*{padding-top:0}</style><style type="text/css">.not-desktop{display:none}.only-mobile,.only-1000px{display:none}</style><link rel="stylesheet" type="text/css" href="./Ashley Edwards_files/style.css"><link rel="stylesheet" type="text/css" href="./Ashley Edwards_files/style-desktop.css"></head>

<body>
    
    <!-- Nav -->
    <nav id="nav">
        <ul>
            <li><a href="https://ashedwards.github.io/#top">Top</a></li>
            <li><a href="https://ashedwards.github.io/#work">Research</a></li>
            <li><a href="cv.pdf">CV</a></li>
        </ul>
    </nav>
    
    <!-- Home -->
    <div class="wrapper wrapper-style1 wrapper-first">
        <article class="container" id="top">
            <div class="row">
                <div class="4u">
                    <span class="me image image-full"><img src="./Ashley Edwards_files/me.jpg" alt=""></span>
                    <center><br>
                        <a href="https://scholar.google.com/citations?user=wmlAA70AAAAJ&hl=en"><img src="scholar.png" width="50" height="50"></a>
                        <a href="https://www.linkedin.com/in/aedwards8/"><img src="linkedin.png" width="50" height="50"></a>
                        <a href="https://github.com/ashedwards/"><img src="github.png" width="50" height="50"></a>
                        <a href="https://twitter.com/RealAshEdwards"><img src="twitter.png" width="50" height="50"></a>
                        </br>
                        Contact: aedwards8 AT gatech.edu
                    </center>
                </div>
                <div class="8u">
                    <header>
                        <h1>Hi. I'm <strong>Ashley Edwards</strong>.</h1>
                    </header>
                    <p align="justify">And I'm a research scientist working on reinforcement learning. I received a PhD in 2019 from Georgia Tech, where I worked on perceptual goal specifications for RL under the guidance of Dr. Charles Isbell. I have spent time at Google Brain as an intern, and most recently as a research scientist at Uber AI Labs. I received my B.S. in Computer Science from the University of Georgia in 2011.
                    
                    <big></big></p><ul style="list-style-type:disc;"><big>
                        <u>News</u>
                        <li> June 2020: Our paper, <a href="https://arxiv.org/pdf/2002.09505.pdf">Estimating Q(s,s') with Deep Deterministic Dynamics Gradients</a>, was accepted into ICML.
                        <li> December 2019: I gave an invited talk at <a href="https://wimlworkshop.org/2019/" target="_blank">WIML</a>.</li>
                        <li> October 2019: I gave an invited talk at WiSDOM's <a href="https://movingtheworldwithdata19.splashthat.com/?fbclid=IwAR3fnBE5k19c1yn7lJcJfIihDwpFN-C-adzsnZKTaYvt5Tql8yvPQwderm4" target="_blank"> Moving the World with Data</a> event.</li>
                        <li> June 2019: I gave an invited talk at the <a href="https://www.re-work.co/events/deep-reinforcement-learning-summit-san-francisco-2019" target="_blank">Rework Deep Reinforcement Learning Summit</a>.</li>
                        <li> May 2019: Our paper, Perceptual Values from Observation, was accepted into the <a href="https://sites.google.com/view/self-supervised-icml2019">Workshop on Self-Supervised Learning</a> at ICML. </li>
                        <li> April 2019: Our paper, <a href="https://arxiv.org/abs/1805.07914">Imitating Latent Policies from Observation</a>, was accepted into ICML. </li>
                        <li> March 2019: I joined Uber AI Labs as a research scientist. </li>
                        <li> January 2019: I succesfully defended my <a href="https://smartech.gatech.edu/handle/1853/61234" target="_blank">dissertation</a>! </li>
                        <li> July 2018: I co-organized the <a href="https://sites.google.com/view/goalsrl/home" target="_blank">1st workshop on Goal Specifications for RL</a> at ICML.</li>
                        <li>May 2018: My work from my internship at Google Brain, <a href="https://arxiv.org/abs/1803.10227" target="_blank">Forward-Backward Reinforcment Learning</a>, was accepted as an oral presentation at the <a href="https://www.cs.unm.edu/amprg/Workshops/MLPC18/index.html" target="_blank">Machine Learning in Planning and Control of Robot Motion
                                workshop</a> at ICRA.</li>
                         </li></big></ul><big>  </big>
                    <p></p>
                    
                    <a href="#work" class="button button-big">Research</a>
                </div>
            </div>
        </article>
    </div>
    
    <!-- Projects -->
    <div class="wrapper wrapper-style2" align="justify">
        <article id="work">
            <header>
                <h2>Research</h2>
                <p align="justify" style="padding: 0 100px">
                
                Rewards often act as the sole feedback for Reinforcement Learning (RL) problems. This signal is surprisingly powerful. It can motivate agents to solve tasks without any further guidance for how to accomplish them. Nevertheless, rewards do not come for free, and are typically hand-engineered for each problem. Furthermore, rewards are often defined as a function of an agentâ€™s state variables. These components have traditionally been tuned to the domain and include information such as the location of the agent or other objects in the world. The reward function then is inherently based on domain-specific representations. While such reward specifications can be sufficient enough to produce optimal behavior, more complex tasks might be difficult to express in this manner. Suppose a robot has a task of building origami figures. The environment would need to provide a reward each time the robot made a correct figure, thus requiring the program designer to define a notion of correctness for each desired configuration. Constructing a reward function for each model might become tedious and even difficultâ€”-what should the inputs even be?
                <br>
                <br>
                Humans regularly exploit learning materials outside of the physical realm of a task, be it through diagrams, videos, text, and speech. For example, we might look at an image of a completed origami figure to determine if our own model is correct. My research describes similar approaches for presenting tasks to agents. In particular, I aim to develop methods for specifying perceptual goals both within and outside of the agentâ€™s environment, and Perceptual Reward Functions (PRFs) that are derived from these goals. This will allow us to represent goals in settings where we can more easily find or construct solutions, without requiring us to modify the reward function when the task changes.
                My thesis aimed to show that employing perceptual goal specifications for goal-directed tasks: is as straightforward as specifying domain-specific rewards; is a more general representation for tasks; and equally enables task completion.
                
                You can view my dissertation <a href="https://smartech.gatech.edu/handle/1853/61234">here</a>!
                </p>
                <hr>
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Estimating Q(s,s') with Deep Deterministic Dynamics Gradients <a href="https://arxiv.org/abs/2002.09505">[paper]</a> <a href="https://github.com/uber-research/D3G">[code]</a></b>
                    <br>Ashley D. Edwards, Himanshu Sahni, Rosanne Liu, Jane Hung, Ankit Jain, Rui Wang, Adrien Ecoffet, Thomas Miconi, Charles Isbell,  Jason Yosinski
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                In this paper, we introduce a novel form of a value function, Q(s, s' ), that expresses the utility of transitioning from a state s to a neighboring state s' and then acting optimally thereafter. In order to derive an optimal policy, we develop a novel forward dynamics model that learns to make next-state predictions that maximize Q(s, s' ). This formulation decouples actions from values while still learning off-policy. We highlight the benefits of this approach in terms of value function transfer, learning within redundant action spaces, and learning off-policy from state observations generated by sub-optimal or completely random policies.
                <br><br>This work was accepted into <a href="https://icml.cc/Conferences/2020">ICML 2020</a>.
                </p>
                <hr>
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Imitating Latent Policies from Observation <a href="https://arxiv.org/abs/1805.07914">[paper]</a> <a href="https://github.com/ashedwards/ILPO">[code]</a></b>
                    <br>Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, Charles L. Isbell
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                In this paper, we describe a novel approach to
                imitation learning that infers latent policies directly from state observations. We introduce a
                method that characterizes the causal effects of latent actions on observations while simultaneously
                predicting their likelihood. We then outline an
                action alignment procedure that leverages a small
                amount of environment interactions to determine
                a mapping between the latent and real-world actions. We show that this corrected labeling can
                be used for imitating the observed behavior, even
                though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that
                it performs better than standard approaches.
                
                <br><br>This work was accepted into <a href="https://icml.cc/Conferences/2019">ICML 2019</a>.
                </p>
                <hr>
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Perceptual Values from Observation <a href="https://arxiv.org/abs/1905.07861">[paper]</a></b>
                    <br>Ashley D. Edwards, Charles L. Isbell
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                Imitation by observation is an approach for learning from expert demonstrations that lack action information, such as videos. Recent approaches to this problem can be placed into two broad categories: training dynamics models that aim to predict the actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). In this paper, we introduce a novel approach that learns values, rather than rewards, directly from observations. We show that by using values, we can significantly speed up RL by removing the need to bootstrap action-values, as compared to sparse-reward specifications.
                
                <br><br>This work was accepted into the <a href="https://sites.google.com/view/self-supervised-icml2019">Workshop on Self-Supervised Learning</a> at ICML 2019.
                </p>
                <hr>
            
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Forward-Backward Reinforcement Learning <a href="https://arxiv.org/abs/1803.10227">[paper]</a></b>
                    <br>Ashley D. Edwards, Laura Downs, James C. Davidson
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                Goals for reinforcement learning problems are typically defined through hand-specified rewards. To design such problems, developers of learning algorithms must inherently be aware of what the task goals are, yet we often require agents to discover them on their own without any supervision beyond these sparse rewards. While much of the power of reinforcement learning derives from the concept that agents can learn with little guidance, this requirement greatly burdens the training process. If we relax this one restriction and endow the agent with knowledge of the reward function, and in particular of the goal, we can leverage backwards induction to accelerate training. To achieve this, we propose training a model to learn to take imagined reversal steps from known goal states. Rather than training an agent exclusively to determine how to reach a goal while moving forwards in time, our approach travels backwards to jointly predict how we got there. We evaluate our work in Gridworld and Towers of Hanoi and empirically demonstrate that it yields better performance than standard DDQN.
                
                <br><br>This work was accepted into the <a href="https://www.cs.unm.edu/amprg/Workshops/MLPC18/index.html">Machine Learning in Planning and Control of Robot Motion
                    workshop</a> at ICRA in 2018.
                
                </p>
                <hr>
                
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Transferring Agent Behaviors from Videos via Motion GANs <a href="https://arxiv.org/abs/1711.07676">[paper]</a></b>
                    <br>Ashley D. Edwards, Charles L. Isbell
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                A major bottleneck for developing general reinforcement learning agents is determining rewards that will yield desirable behaviors under various circumstances. We introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels. In particular, we train a generative adversarial network to produce short sub-goals represented through motion templates. We demonstrate that this approach generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions can be used to train reinforcement learning agents.
                
                <br><br>This work was accepted into the <a href="https://sites.google.com/view/deeprl-symposium-nips2017">Deep Reinforcement Learning Symposium</a> at NIPS in 2017.
                
                </p>
                <hr>
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Cross-Domain Perceptual Reward Functions <a href="https://arxiv.org/pdf/1705.09045.pdf">[paper]</a></b>
                    <br>Ashley D. Edwards, Srijan Sood, Charles L. Isbell
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agents environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agents. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agents state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning.
                
                <br><br>This work was accepted into <a href="http://rldm.org/rldm2017/">RLDM 2017</a>.
                </p>
                <hr>
                
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Perceptual Reward Functions <a href="https://www.cc.gatech.edu/~aedwards/ijcai.pdf">[paper]</a></b>
                    <br>Ashley D. Edwards, Charles L. Isbell, Atsuo Takanishi
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                Reinforcement learning problems are often described
                through rewards that indicate if an agent
                has completed some task. This specification can
                yield desirable behavior, however many problems
                are difficult to specify in this manner, as one often
                needs to know the proper configuration for the
                agent. When humans are learning to solve tasks,
                we often learn from visual instructions composed
                of images or videos. Such representations motivate
                our development of Perceptual Reward Functions,
                which provide a mechanism for creating visual task
                descriptions. We show that this approach allows an
                agent to learn from rewards that are based on raw
                pixels rather than internal parameters.
                
                <br><br>This work was accepted into a 2016 IJCAI workshop, <a href="https://sites.google.com/site/deeprlijcai16/home">Deep Reinforcement Learning: Frontiers and Challenges</a>.
                </p>
                <hr>
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Expressing Tasks Robustly via Multiple Discount Factors <a href="https://www.cc.gatech.edu/~aedwards/RLDM.pdf">[paper]</a></b>
                    <br>Ashley D. Edwards, Michael L. Littman, Charles L. Isbell
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                Reward engineering is the problem of expressing a target task for an agent in the form of rewards for a Markov decision process.
                To be useful for learning, it is important that these encodings be robust to structural changes in the underlying domain; that is, the
                specification remain unchanged for any domain in some target class. We identify problems that are difficult to express robustly via the
                standard model of discounted rewards. In response, we examine the idea of decomposing a reward function into separate components,
                each with its own discount factor. We describe a method for finding robust parameters through the concept of task engineering, which
                additionally modifies the discount factors. We present a method for optimizing behavior in this setting and show that it could provide
                a more robust language than standard approaches.
                <br><br>This work was accepted into <a href="http://rldm.org/rldm2015/" target="_blank">RLDM 2015</a>.
                </p>
                <hr>
                <!-- <p align = "justify" style="padding: 0 100px">
                 </br><b><center>Interactive Clustering for Labeling Groups in Phones (Spring 2012 - Fall 2012)</b>
                 </br></center></p><p align = "justify" style="padding: 0 100px">
                 Groups of contacts created within mobile phone interfaces enable users to easily interact with a specific subset of
                 people from their social networks. However, manually organizing contacts may become difficult for users as their
                 contact lists grow. We introduce a method that attempts to determine a userâ€™s mobile groups by interactively
                 clustering contacts from their cell phone. We will show that by actively querying the user on likely mislabeled
                 contacts, and then re-clustering the contacts after a correct label is received, we reduce the number of contacts that
                 must be manually labeled.
                 </br></br>You can find the final report for this work <a href = "samsung.pdf">here</a>.
                 </p> -->
                
                <!-- </header> -->
                
                <!-- <footer>
                 <a href="#projects" class="button button-big">More projects I've worked on</a>
                 </footer> -->
                
                
                
                <!-- Projects -->
                <!-- <div class="wrapper wrapper-style2" align="justify">
                 <article id="projects">
                 <header>
                 <h2>Projects</h2>
                 <hr>
                 <p align = "justify" style="padding: 0 100px">
                 <center><b>Context-Based Messages on Google Glass (Spring 2015)</b>
                 <i></br>Joint work with <a href="http://www.cc.gatech.edu/~hsahni3/"  target="_blank">Himanshu Sahni</a> and <a href = "http://www.cc.gatech.edu/~alau8/" target="_blank">Andrea Lau</a></i></center></p>
                 
                 <center><iframe width="560" height="315" src="https://www.youtube.com/embed/Fr2NrZfEHkM" frameborder="0" allowfullscreen></iframe></center>
                 </br>
                 <p align = "justify" style="padding: 0 100px">
                 Context Messages is a system that allows the user to send
                 messages when they are most relevant to the receiverâ€™s context.
                 The context could be based on location, weather, time,
                 availability, activities being performed, traffic, etc. We describe
                 an implementation of the system using some of these
                 contexts on the Google Glass platform. We conducted user
                 studies to evaluate the easy of use and learnability of the system.
                 The system received an average score of 6.3 out of a 7
                 point Likert scale in terms of learnabilty with a small variation
                 of 0.23 points between users. From the user study, we
                 uncovered concerns of privacy and of false triggering of menu
                 prompts. We introduce a privacy web interface and redesign
                 our menu system to address these concerns.
                 </br></br>You can find the final class report for this work <a href = "muc.pdf">here</a>.
                 </p>
                 <hr>
                 <p align = "justify" style="padding: 0 100px">
                 <center><b>Task Planning using Semantic Mapping and Human Feedback (Fall 2012)</b>
                 </br><i>Joint work with <a href="http://www.cc.gatech.edu/grads/k/kbullard/" target="_blank">Kalesha Bullard</a>, Gabino Dabdoub, and Salim Dabdoub</i></center></p><p align = "justify" style="padding: 0 100px">
                 Robotic assistants are becoming increasingly useful
                 in many different domains, such as in the healthcare industry
                 for assisting with elderly or disabled patients, in outer space for
                 scientific exploration purposes, in the home for personal
                 assistance, and in an office setting. If robots are to act as
                 assistants or companions to humans, a human should be able to
                 ask a robot to provide support by performing some higher-level
                 task, just as they would ask another human, and the robot
                 should be able to respond accordingly. Our project seeks to
                 build an intelligent reasoning and planning framework using a
                 semantic map as the underlying knowledge representation, and
                 integrates human-robot interaction in order to minimize the
                 uncertainty of the robot. A robot assistant is given a higherlevel
                 task to achieve by a human. We explore how a semantic
                 map may be used, along with human feedback, in order to help
                 the robot interpret its task, reason about how to achieve the
                 task in the given environment, and subsequently plan and
                 execute the task effectively. The robot re-plans dynamically as
                 it acquires new information.
                 </br></br>The final class report for this work can be found <a href="rip.pdf" target="_blank">here</a>.
                 </p>
                 <hr>
                 <p align = "justify" style="padding: 0 100px">
                 <center><b>COBOT (Fall 2012)</b>
                 </br><i>Joint work with <a href="http://www.cc.gatech.edu/~hsahni3/" target="_blank">Himanshu Sahni</a></i></center></p><p align = "justify" style="padding: 0 100px">
                 Twitter is a popular social network where users frequently post short
                 messages and engage in conversations with other users. This provides a
                 large amount of information about the userâ€™s interests and social
                 preferences and has been a lucrative research area. In particular, research
                 in Social Search, the act of posing questions to and receiving answers from
                 oneâ€™s social network, has gained interest. However, the large database of
                 information that results from usersâ€™ Twitter interactions is not presently
                 leveraged for aiding in social search. In this paper, we introduce CoBot, a
                 novel social assistant and data mining agent for Twitter. CoBot gathers
                 social statistics about users and their followers, allows them to query about
                 their social interactions, and assists with Social Search by recommending
                 knowledgeable followers and providing relevant tweets. By using social
                 statistics about users and learning from feedback about recommendations,
                 CoBot is more amenable to â€˜social queriesâ€™ than conventional search
                 engines.
                 
                 </br></br>The final class report for this work can be found <a href="cobot.pdf" target="_blank">here</a>.
                 </p>
                 <hr>
                 <p align = "justify" style="padding: 0 100px">
                 <center><b>Creating Adaptive Agents through Interaction (Spring 2012)</b>
                 </br><i>Joint work with <a href="http://www.cc.gatech.edu/~hsahni3/" target="_blank">Himanshu Sahni</a></i></center></p><p align = "justify" style="padding: 0 100px">
                 Software and user interfaces are designed with
                 the preferences of the end-users in mind.
                 However, these modules are not easily
                 modified. Users often think of changes to
                 products long before they are implemented,
                 but changing these products requires constant
                 updates and resources.
                 We propose a method for creating an agent
                 that is capable of learning and adapting from
                 interactions with an end user. In our approach,
                 we use Interactive Reinforcement Learning to
                 train a bipedal agent to learn how to run and
                 remain balanced. This will give the user a
                 unique experience with the game, while
                 allowing the agent to adapt to the users
                 preferences.
                 
                 </br></br>Our poster for this work can be found <a href="poster.pdf">here</a>.
                 </p>
                 <hr>
                 </header>-->
                
                
                <ul id="copyright">
                    <li>Design: <a href="http://html5up.net/">HTML5 UP</a></li>
                </ul>
                </footer>
</body></html>
